---
phase: 05-wire-scheduling-deduplication-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/index.ts
autonomous: true
requirements:
  - DATA-01
  - DATA-02
  - DATA-03
  - DATA-04
  - DATA-06

must_haves:
  truths:
    - "Application startup launches BullMQ worker process"
    - "setupCrawlJobs() is called at startup, registering ticketmaster-daily, axs-daily, dice-daily, venues-daily repeating jobs"
    - "setupCleanupJob() is called at startup, registering weekly Sunday 4 AM cleanup"
    - "If Redis is unavailable, API server still starts and serves requests (scheduling failure is non-fatal)"
  artifacts:
    - path: "src/index.ts"
      provides: "Application entry point with scheduling wiring"
      contains: "createWorker"
    - path: "src/index.ts"
      provides: "Application entry point with scheduling wiring"
      contains: "setupCrawlJobs"
    - path: "src/index.ts"
      provides: "Application entry point with scheduling wiring"
      contains: "setupCleanupJob"
  key_links:
    - from: "src/index.ts"
      to: "src/scheduling/processors.ts"
      via: "createWorker() import and call"
      pattern: "createWorker"
    - from: "src/index.ts"
      to: "src/scheduling/jobs.ts"
      via: "setupCrawlJobs() and setupCleanupJob() import and call"
      pattern: "setupCrawlJobs|setupCleanupJob"
---

<objective>
Wire BullMQ worker startup and job registration into the application entry point so crawl jobs execute automatically on schedule.

Purpose: Currently `src/index.ts` starts only the Fastify API server. The BullMQ worker (`createWorker()`), crawl job registration (`setupCrawlJobs()`), and cleanup job registration (`setupCleanupJob()`) are fully implemented in `src/scheduling/` but have zero call sites. Crawlers never execute automatically. This plan wires those three calls into `src/index.ts`.

Output: Modified `src/index.ts` that calls `createWorker()`, `setupCrawlJobs()`, and `setupCleanupJob()` after the API server starts, wrapped in a try/catch so Redis unavailability does not prevent the API from starting.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-wire-scheduling-deduplication-pipeline/05-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire scheduling startup into src/index.ts</name>
  <files>src/index.ts</files>
  <action>
Modify `src/index.ts` (currently 31 lines) to import and call the three scheduling functions after the server starts.

Current `src/index.ts` structure:
- Lines 1-2: imports (buildServer, db)
- Lines 12-26: main() function — builds server, calls server.listen(), logs port
- Lines 28-31: main().catch() handler

Changes to make:

1. Add two new imports after the existing imports (after line 2):
   ```typescript
   import { setupCrawlJobs, setupCleanupJob } from './scheduling/jobs.js';
   import { createWorker } from './scheduling/processors.js';
   ```

2. After `server.log.info(...)` on line 21, add scheduling startup wrapped in try/catch:
   ```typescript
   // Start BullMQ worker and register repeating jobs
   // Wrapped in try/catch: Redis unavailability must NOT prevent API from starting
   try {
     const worker = createWorker();
     await setupCrawlJobs();
     await setupCleanupJob();
     server.log.info('Scheduling worker started, crawl jobs registered');
   } catch (error) {
     server.log.error(
       { error },
       'Scheduling worker failed to start — crawls will not run automatically'
     );
     // Do NOT rethrow — API should still serve requests even if Redis is down
   }
   ```

Do NOT remove the `db` import (line 2) — it may be used elsewhere. Do NOT modify the catch block of the outer try/catch (lines 22-25). Do NOT call setupCrawlJobs() from inside the outer try/catch's catch block.

Anti-pattern to avoid: Do NOT wrap `createWorker()` in the outer try/catch (lines 13-25) that also wraps server startup — if Redis fails, it would prevent the server from starting. The scheduling try/catch must be INSIDE `main()` but AFTER `server.listen()` succeeds.
  </action>
  <verify>
Run TypeScript compilation check: `npx tsc --noEmit`

Expected: zero TypeScript errors for src/index.ts.

Also verify the three function names appear in the file:
- `grep -n "createWorker\|setupCrawlJobs\|setupCleanupJob" src/index.ts` should return 5 lines (2 import + 3 call sites).
  </verify>
  <done>
`src/index.ts` compiles without TypeScript errors. The file contains imports for `setupCrawlJobs`, `setupCleanupJob` (from `./scheduling/jobs.js`) and `createWorker` (from `./scheduling/processors.js`). All three are called after `server.listen()` succeeds. The calls are wrapped in a try/catch that logs but does not rethrow on failure.
  </done>
</task>

</tasks>

<verification>
1. TypeScript compilation: `npx tsc --noEmit` — must exit 0 with no errors
2. Import check: `grep -n "createWorker\|setupCrawlJobs\|setupCleanupJob" src/index.ts` — must return 5 lines
3. Structure check: scheduling try/catch must appear AFTER the `server.listen()` call, not before it
4. Safety check: the scheduling catch block must NOT rethrow the error (API should start even if Redis is down)
</verification>

<success_criteria>
- `src/index.ts` imports and calls `createWorker()`, `setupCrawlJobs()`, and `setupCleanupJob()` after server start
- Scheduling startup errors are caught and logged without crashing the API server
- `npx tsc --noEmit` exits 0 with no errors on modified file
- DATA-01 through DATA-04 and DATA-06 are now wired: crawl and cleanup jobs will execute automatically when the application starts with a working Redis connection
</success_criteria>

<output>
After completion, create `.planning/phases/05-wire-scheduling-deduplication-pipeline/05-01-SUMMARY.md`
</output>
