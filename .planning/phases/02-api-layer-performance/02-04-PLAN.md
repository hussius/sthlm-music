---
phase: 02-api-layer-performance
plan: 04
type: execute
wave: 3
depends_on: ["02-03"]
files_modified:
  - src/scripts/seed-test-data.ts
  - src/scripts/load-test.ts
  - package.json
autonomous: true
requirements: [PERF-01, PERF-02]

must_haves:
  truths:
    - "Database can be seeded with 10,000+ test events"
    - "Load test runs successfully against /api/events endpoint"
    - "API maintains sub-200ms average response time under load"
    - "API handles concurrent requests without degradation"
    - "p95 response time stays under 300ms"
    - "GIN trigram indexes are used for text search queries"
  artifacts:
    - path: "src/scripts/seed-test-data.ts"
      provides: "Test data generation for load testing"
      exports: ["seedTestData"]
      min_lines: 80
    - path: "src/scripts/load-test.ts"
      provides: "Autocannon load testing script"
      exports: []
      min_lines: 60
  key_links:
    - from: "src/scripts/seed-test-data.ts"
      to: "src/db/client.ts"
      via: "database insert"
      pattern: "db\\.insert.*events"
    - from: "src/scripts/load-test.ts"
      to: "autocannon"
      via: "HTTP load testing"
      pattern: "import.*autocannon"
---

<objective>
Create test data seeding script and load testing suite to validate sub-200ms response time requirement with 10,000+ events in database.

Purpose: Prove API meets performance requirements (PERF-01, PERF-02) under realistic load conditions before considering complete. Validate that GIN trigram indexes and cursor pagination deliver expected performance.

Output: Seed script generating 10,000+ realistic events and load testing script measuring response times across all filter types with performance report.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-api-layer-performance/02-RESEARCH.md
@.planning/phases/02-api-layer-performance/02-01-SUMMARY.md
@.planning/phases/02-api-layer-performance/02-02-SUMMARY.md
@.planning/phases/02-api-layer-performance/02-03-SUMMARY.md
@src/db/schema.ts
@src/db/client.ts
@src/normalization/genre-mappings.ts
@src/normalization/venue-mappings.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test data seeding script</name>
  <files>
    src/scripts/seed-test-data.ts
    package.json
  </files>
  <action>
Create src/scripts/seed-test-data.ts to generate 10,000+ realistic events:

**Imports:**
- Import db from '../db/client.js'
- Import events, NewEvent from '../db/schema.js'
- Import CANONICAL_GENRES from '../normalization/genre-mappings.js'
- Import VENUE_MAPPINGS from '../normalization/venue-mappings.js'

**Data generation:**

1. **Artist pool (500 names):**
   - Mix of real bands and generated names
   - Examples: "Metallica", "Iron Maiden", "The Beatles", "Pink Floyd", etc.
   - Use array of strings for consistency

2. **Venue pool:**
   - Extract venue names from VENUE_MAPPINGS (13 Stockholm venues)
   - Use normalized names: "Debaser Strand", "Fasching", "Slaktkyrkan", etc.

3. **Event generation function:**
   - Generate date: random date within next 12 months
   - Generate time: random hour 18:00-23:00
   - Generate name: `${artist} Live in Stockholm` or `${artist} ${year} Tour`
   - Generate genre: random from CANONICAL_GENRES
   - Generate ticketSources: array with 1-3 random platforms (ticketmaster, axs, dice, venue-direct)
   - Generate price: random 150-650 SEK or null (20% free events)
   - Generate sourceId: random UUID
   - Generate sourcePlatform: random from ticket platforms

4. **Seeding logic:**
   - Accept count parameter (default 10000)
   - Generate events in batches of 100 (avoid memory issues)
   - Use db.insert(events).values(batch).onConflictDoNothing() to handle duplicates
   - Log progress every 1000 events
   - Return total inserted count

5. **Main execution:**
   - If run directly (import.meta.url check), call seedTestData()
   - Close database connection after completion
   - Log summary: "Seeded X events in Y seconds"

Add npm script to package.json:
```json
"seed:test": "tsx src/scripts/seed-test-data.ts"
```
  </action>
  <verify>
Run TypeScript compilation: npm run build
Check src/scripts/seed-test-data.ts exists
Run seeding script: npm run seed:test
Monitor console output for progress logs
Query database to verify count: docker exec -it $(docker ps -q -f name=postgres) psql -U postgres -d events -c "SELECT COUNT(*) FROM events;"
Verify count is 10,000+
  </verify>
  <done>
- Seed script generates 10,000+ realistic events
- Events distributed across 13 venues and 11 genres
- Date range covers next 12 months
- npm run seed:test successfully populates database
- Progress logging shows batches processed
  </done>
</task>

<task type="auto">
  <name>Task 2: Create load testing script with autocannon</name>
  <files>
    src/scripts/load-test.ts
    package.json
  </files>
  <action>
Install autocannon:
```bash
npm install --save-dev autocannon @types/autocannon
```

Create src/scripts/load-test.ts following pattern from 02-RESEARCH.md section "Load Testing with Autocannon":

**Test configuration:**
- URL: http://localhost:3000
- Connections: 100 concurrent
- Duration: 30 seconds
- Title: 'Events API Load Test'

**Test scenarios (requests array):**
1. Basic pagination: GET /api/events?limit=20
2. Genre filter: GET /api/events?genre=rock&limit=20
3. Date range filter: GET /api/events?dateFrom=2024-03-01T00:00:00Z&dateTo=2024-03-31T23:59:59Z&limit=20
4. Venue filter: GET /api/events?venue=Debaser%20Strand&limit=20
5. Artist search: GET /api/events?artistSearch=metal&limit=20
6. Event search: GET /api/events?eventSearch=live&limit=20
7. Combined filters: GET /api/events?genre=electronic&dateFrom=2024-06-01T00:00:00Z&limit=20

**Script structure:**
- Import autocannon and types
- Define async function runLoadTest()
- Run autocannon with configuration
- Print results using autocannon.printResult()
- Check performance targets:
  - Average latency < 200ms (target met)
  - p95 latency < 300ms (acceptable)
  - p99 latency < 500ms (acceptable)
- Log PASS/FAIL for each metric
- Exit with code 1 if targets not met

**Main execution:**
- If run directly, call runLoadTest()
- Handle errors and log clearly

Add npm script to package.json:
```json
"test:load": "tsx src/scripts/load-test.ts"
```

**Important:** Script assumes server is running. Document in comments: "Start server with 'npm run dev' before running load test."
  </action>
  <verify>
Start server: npm run dev (in separate terminal)
Wait for server to be ready
Run load test: npm run test:load
Monitor console output for autocannon results
Check performance metrics:
- Average latency
- p95 latency
- p99 latency
- Requests per second
Verify PASS/FAIL output for targets
Stop server after test completes
  </verify>
  <done>
- Load test script tests all filter types
- Autocannon runs 30-second test with 100 concurrent connections
- Performance metrics calculated (avg, p50, p95, p99)
- Script validates sub-200ms average response time
- npm run test:load executes successfully
- Results show whether performance targets are met
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify index usage with EXPLAIN ANALYZE</name>
  <files>
    src/scripts/verify-indexes.ts
    package.json
  </files>
  <action>
Create src/scripts/verify-indexes.ts to verify PostgreSQL is using indexes correctly:

**Script purpose:** Run EXPLAIN ANALYZE on representative queries to confirm indexes are used, not sequential scans.

**Queries to test:**
1. Date range query: `SELECT * FROM events WHERE date >= NOW() AND date <= NOW() + INTERVAL '1 month' LIMIT 20`
2. Genre query: `SELECT * FROM events WHERE genre = 'rock' LIMIT 20`
3. Artist text search: `SELECT * FROM events WHERE artist ILIKE '%metal%' LIMIT 20`
4. Event name search: `SELECT * FROM events WHERE name ILIKE '%live%' LIMIT 20`

**Implementation:**
- Import db from '../db/client.js'
- Import sql from 'drizzle-orm'
- For each query:
  - Run EXPLAIN ANALYZE via db.execute(sql.raw(`EXPLAIN ANALYZE ${query}`))
  - Parse result to check for index usage
  - Look for "Index Scan" or "Bitmap Index Scan" in plan
  - Flag if "Seq Scan" appears (sequential scan = bad performance)
- Log results in table format
- Exit with code 1 if any query uses sequential scan

Add npm script:
```json
"verify:indexes": "tsx src/scripts/verify-indexes.ts"
```

**Success criteria:**
- Date range uses date_idx (B-tree index)
- Genre uses genre_idx (B-tree index)
- Artist search uses idx_events_artist_trgm (GIN trigram index)
- Event name search uses idx_events_name_trgm (GIN trigram index)
  </action>
  <verify>
Run index verification: npm run verify:indexes
Check console output for index usage
Verify no "Seq Scan" warnings
Verify GIN trigram indexes used for ILIKE queries
Verify B-tree indexes used for exact matches and ranges
  </verify>
  <done>
- Index verification script checks EXPLAIN ANALYZE output
- Confirms GIN trigram indexes used for text search
- Confirms B-tree indexes used for filters
- Script exits with error if sequential scans detected
- npm run verify:indexes validates database performance
  </done>
</task>

</tasks>

<verification>
1. Seed database:
   - npm run seed:test creates 10,000+ events
   - Check database count matches expected

2. Run load test:
   - Start server: npm run dev
   - Run load test: npm run test:load
   - Verify average latency < 200ms
   - Verify p95 latency < 300ms
   - Check requests/second throughput

3. Verify indexes:
   - npm run verify:indexes shows index usage
   - No sequential scans on indexed columns

4. Manual testing:
   - Test various filter combinations
   - Check X-Response-Time header values
   - Verify all requests complete quickly

5. Performance validation:
   - All filter types return within 200ms average
   - No degradation with 10,000+ events
   - Cursor pagination performs consistently
</verification>

<success_criteria>
- Database can be seeded with 10,000+ events via npm script
- Load test runs successfully with 100 concurrent connections
- Average response time under 200ms across all filter types
- p95 response time under 300ms
- GIN trigram indexes used for text search (verified via EXPLAIN ANALYZE)
- B-tree indexes used for exact match and range filters
- API handles load without errors or degradation
- Phase 2 performance requirements (PERF-01, PERF-02) validated and met
</success_criteria>

<output>
After completion, create `.planning/phases/02-api-layer-performance/02-04-SUMMARY.md` using the summary template.
</output>
