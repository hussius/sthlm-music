---
phase: 01-data-foundation-multi-platform-aggregation
plan: 06
type: execute
wave: 3
depends_on:
  - 01-01
  - 01-02
files_modified:
  - src/crawlers/venues/base-venue-crawler.ts
  - src/crawlers/venues/kollektivet-livet.ts
  - src/crawlers/venues/slaktkyrkan.ts
  - src/crawlers/venues/hus7.ts
  - src/crawlers/venues/fasching.ts
  - src/crawlers/venues/nalen.ts
  - src/crawlers/venues/fylkingen.ts
  - src/crawlers/venues/slakthuset.ts
  - src/crawlers/venues/fallan.ts
  - src/crawlers/venues/landet.ts
  - src/crawlers/venues/mosebacke.ts
  - src/crawlers/venues/kagelbanan.ts
  - src/crawlers/venues/pet-sounds.ts
  - src/crawlers/venues/debaser.ts
  - src/crawlers/venues/health-check.ts
autonomous: true
requirements:
  - DATA-04

must_haves:
  truths:
    - System scrapes all 13 priority venue websites directly
    - Each venue has dedicated crawler with venue-specific selectors
    - Health checks detect when venue website structure changes
    - Venue events are normalized and stored in database
  artifacts:
    - path: "src/crawlers/venues/base-venue-crawler.ts"
      provides: "Base crawler template for venue websites"
      exports: ["BaseVenueCrawler", "VenueConfig"]
      min_lines: 80
    - path: "src/crawlers/venues/kollektivet-livet.ts"
      provides: "Kollektivet Livet venue crawler"
      exports: ["crawlKollektivetLivet"]
      min_lines: 40
    - path: "src/crawlers/venues/health-check.ts"
      provides: "Health monitoring for venue crawlers"
      exports: ["checkVenueHealth", "VenueHealthStatus"]
      min_lines: 60
  key_links:
    - from: "src/crawlers/venues/kollektivet-livet.ts"
      to: "src/crawlers/venues/base-venue-crawler.ts"
      via: "extends base crawler template"
      pattern: "extends BaseVenueCrawler"
    - from: "src/crawlers/venues/base-venue-crawler.ts"
      to: "crawlee"
      via: "uses CheerioCrawler for static sites"
      pattern: "CheerioCrawler"
    - from: "src/crawlers/venues/health-check.ts"
      to: "src/storage/event-repository.ts"
      via: "checks stored event counts"
      pattern: "countEvents"
---

<objective>
Implement direct website crawlers for 13 priority Stockholm venues to capture events that may not appear on major ticketing platforms. Each venue has a custom crawler adapted to its website structure, with health monitoring to detect structural changes.

Purpose: Priority venues (Kollektivet Livet, Slaktkyrkan, etc.) often sell tickets directly through their own websites. These events may not appear on Ticketmaster, AXS, or DICE, creating gaps in coverage. Direct venue crawling ensures comprehensive event capture.

Output: Base venue crawler template, 13 venue-specific crawlers, health check monitoring system, and integrated database storage.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-data-foundation-multi-platform-aggregation/01-RESEARCH.md
@.planning/phases/01-data-foundation-multi-platform-aggregation/01-01-SUMMARY.md
@.planning/phases/01-data-foundation-multi-platform-aggregation/01-02-SUMMARY.md

Priority venues from PROJECT.md:
1. Kollektivet Livet
2. Slaktkyrkan
3. Hus 7
4. Fasching
5. Nalen
6. Fylkingen
7. Slakthuset
8. Fållan
9. Landet
10. Mosebacke
11. Kägelbanan
12. Pet Sounds
13. Debaser
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create base venue crawler template and configure all 13 venues</name>
  <files>
    src/crawlers/venues/base-venue-crawler.ts
    src/crawlers/venues/venue-configs.ts
  </files>
  <action>
Create reusable base crawler following RESEARCH.md Pattern 1 (Crawlee Strategy Pattern):

1. Define VenueConfig interface:
   ```typescript
   export interface VenueConfig {
     name: string;
     url: string;
     selectors: {
       eventContainer: string;
       eventName: string;
       eventDate: string;
       eventTime?: string;
       eventGenre?: string;
       eventPrice?: string;
       eventUrl: string;
     };
     usesJavaScript: boolean;  // true = Playwright, false = Cheerio
     dateFormat?: string;  // e.g., 'dd/MM/yyyy' for parsing
   }
   ```

2. Create BaseVenueCrawler class:
   ```typescript
   import { CheerioCrawler, PlaywrightCrawler, log } from 'crawlee';
   import { transformVenueEvent } from '../../normalization/transformers.js';
   import { upsertEvent } from '../../storage/event-repository.js';

   export class BaseVenueCrawler {
     constructor(private config: VenueConfig) {}

     async crawl(): Promise<{ success: number; failed: number }> {
       let success = 0;
       let failed = 0;

       // Choose crawler based on whether site uses JavaScript
       const CrawlerClass = this.config.usesJavaScript ? PlaywrightCrawler : CheerioCrawler;

       const crawler = new CrawlerClass({
         maxConcurrency: 1,  // Be gentle with small venue sites
         requestHandlerTimeoutSecs: 30,

         requestHandler: async (context: any) => {
           const { $, page, request } = context;

           log.info(`Crawling ${this.config.name}: ${request.url}`);

           // Wait for content if using Playwright
           if (page) {
             await page.waitForSelector(this.config.selectors.eventContainer, {
               timeout: 10000
             }).catch(() => {
               log.warning(`Event container not found for ${this.config.name}`);
             });
           }

           // Extract events (handle both Cheerio and Playwright)
           let eventElements;
           if ($) {
             // Cheerio path
             eventElements = $(this.config.selectors.eventContainer);
           } else if (page) {
             // Playwright path
             eventElements = await page.$$(this.config.selectors.eventContainer);
           }

           if (!eventElements || eventElements.length === 0) {
             log.warning(`No events found for ${this.config.name}`);
             return;
           }

           // Extract data from each event
           const rawEvents = [];
           if ($) {
             // Cheerio extraction
             eventElements.each((i, el) => {
               const $el = $(el);
               rawEvents.push({
                 name: $el.find(this.config.selectors.eventName).text().trim(),
                 date: $el.find(this.config.selectors.eventDate).text().trim(),
                 time: this.config.selectors.eventTime ?
                   $el.find(this.config.selectors.eventTime).text().trim() : undefined,
                 genre: this.config.selectors.eventGenre ?
                   $el.find(this.config.selectors.eventGenre).text().trim() : undefined,
                 price: this.config.selectors.eventPrice ?
                   $el.find(this.config.selectors.eventPrice).text().trim() : undefined,
                 url: $el.find(this.config.selectors.eventUrl).attr('href')
               });
             });
           } else if (page) {
             // Playwright extraction
             for (const el of eventElements) {
               const name = await el.$eval(this.config.selectors.eventName, (e: any) => e.textContent?.trim());
               const date = await el.$eval(this.config.selectors.eventDate, (e: any) => e.textContent?.trim());
               const urlEl = await el.$(this.config.selectors.eventUrl);
               const url = urlEl ? await urlEl.getAttribute('href') : null;

               rawEvents.push({ name, date, url, venue: this.config.name });
             }
           }

           log.info(`Extracted ${rawEvents.length} events from ${this.config.name}`);

           // Normalize and save events
           for (const rawEvent of rawEvents) {
             if (!rawEvent.name || !rawEvent.url) {
               failed++;
               continue;
             }

             // Make URL absolute if relative
             let fullUrl = rawEvent.url;
             if (fullUrl && !fullUrl.startsWith('http')) {
               const baseUrl = new URL(this.config.url);
               fullUrl = new URL(fullUrl, baseUrl.origin).href;
             }

             const normalized = transformVenueEvent({
               name: rawEvent.name,
               artist: rawEvent.name,  // Venue events often don't separate artist/event
               venue: this.config.name,
               date: rawEvent.date,
               genre: rawEvent.genre || 'other',
               url: fullUrl,
               price: rawEvent.price,
               id: `${this.config.name.toLowerCase().replace(/\s+/g, '-')}-${Date.now()}`
             });

             if (!normalized.success) {
               log.warning(`Failed to normalize event from ${this.config.name}:`, normalized.errors);
               failed++;
               continue;
             }

             try {
               await upsertEvent(normalized.data);
               success++;
             } catch (error) {
               log.error(`Failed to save event from ${this.config.name}:`, error);
               failed++;
             }
           }
         }
       });

       await crawler.run([this.config.url]);

       log.info(`${this.config.name} crawl complete: ${success} success, ${failed} failed`);
       return { success, failed };
     }
   }
   ```

3. Create venue-configs.ts with all 13 venues:
   ```typescript
   export const VENUE_CONFIGS: VenueConfig[] = [
     {
       name: 'Kollektivet Livet',
       url: 'https://kollektivetlivet.se/evenemang',  // Verify URL
       selectors: {
         eventContainer: '.event-item, .event, article',
         eventName: '.event-title, h2, h3',
         eventDate: '.event-date, time',
         eventUrl: 'a'
       },
       usesJavaScript: false
     },
     {
       name: 'Slaktkyrkan',
       url: 'https://slaktkyrkan.com/program',  // Verify URL
       selectors: {
         eventContainer: '.event-card, .event',
         eventName: '.title, h2',
         eventDate: '.date, time',
         eventUrl: 'a'
       },
       usesJavaScript: false
     },
     // Add remaining 11 venues with placeholder URLs and selectors
     // These will need to be refined during execution
     // ...
   ];
   ```

Note: Venue website URLs and selectors are PLACEHOLDERS and MUST be verified/updated during execution. Each venue has unique HTML structure requiring custom selector configuration.
  </action>
  <verify>
Test base crawler with one venue:
  cat > test-venue-crawler.ts << 'EOF'
import { BaseVenueCrawler } from './src/crawlers/venues/base-venue-crawler.js';
import { VenueConfig } from './src/crawlers/venues/base-venue-crawler.js';

// Test with simple venue (adjust selectors based on actual page)
const testConfig: VenueConfig = {
  name: 'Test Venue',
  url: 'https://kollektivetlivet.se/evenemang',
  selectors: {
    eventContainer: '.event',
    eventName: 'h2',
    eventDate: 'time',
    eventUrl: 'a'
  },
  usesJavaScript: false
};

const crawler = new BaseVenueCrawler(testConfig);
const result = await crawler.crawl();

console.log('Result:', result);
console.log('Base crawler works:', result.success >= 0 ? 'PASS' : 'FAIL');
EOF

  tsx test-venue-crawler.ts
  Expected: Crawler runs without crashing (may have 0 success if selectors wrong, but should not error)
  </verify>
  <done>
- BaseVenueCrawler supports both Cheerio (static) and Playwright (dynamic) modes
- VenueConfig interface defines all required configuration
- venue-configs.ts contains configuration for all 13 venues
- Base crawler extracts events and normalizes using transformVenueEvent
- Relative URLs converted to absolute for ticket links
- Error handling ensures one failed venue doesn't stop others
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement individual venue crawlers and refine selectors</name>
  <files>
    src/crawlers/venues/kollektivet-livet.ts
    src/crawlers/venues/slaktkyrkan.ts
    src/crawlers/venues/hus7.ts
    src/crawlers/venues/fasching.ts
    src/crawlers/venues/nalen.ts
    src/crawlers/venues/fylkingen.ts
    src/crawlers/venues/slakthuset.ts
    src/crawlers/venues/fallan.ts
    src/crawlers/venues/landet.ts
    src/crawlers/venues/mosebacke.ts
    src/crawlers/venues/kagelbanan.ts
    src/crawlers/venues/pet-sounds.ts
    src/crawlers/venues/debaser.ts
    src/crawlers/venues/index.ts
  </files>
  <action>
Create wrapper functions for each venue and refine configurations:

1. For EACH of the 13 venues, create a file (e.g., kollektivet-livet.ts):
   ```typescript
   import { BaseVenueCrawler, VenueConfig } from './base-venue-crawler.js';

   const config: VenueConfig = {
     name: 'Kollektivet Livet',
     url: 'https://kollektivetlivet.se/evenemang',  // VERIFY during execution
     selectors: {
       eventContainer: '.event-item',  // REFINE during execution
       eventName: '.event-title',
       eventDate: '.event-date',
       eventUrl: 'a.event-link'
     },
     usesJavaScript: false
   };

   export async function crawlKollektivetLivet() {
     const crawler = new BaseVenueCrawler(config);
     return crawler.crawl();
   }
   ```

2. Create venues/index.ts that exports all crawlers:
   ```typescript
   export { crawlKollektivetLivet } from './kollektivet-livet.js';
   export { crawlSlaktkyrkan } from './slaktkyrkan.js';
   export { crawlHus7 } from './hus7.js';
   export { crawlFasching } from './fasching.js';
   export { crawlNalen } from './nalen.js';
   export { crawlFylkingen } from './fylkingen.js';
   export { crawlSlakthuset } from './slakthuset.js';
   export { crawlFallan } from './fallan.js';
   export { crawlLandet } from './landet.js';
   export { crawlMosebacke } from './mosebacke.js';
   export { crawlKagelbanan } from './kagelbanan.js';
   export { crawlPetSounds } from './pet-sounds.js';
   export { crawlDebaser } from './debaser.js';

   // Convenience function to crawl all venues
   export async function crawlAllVenues() {
     const crawlers = [
       crawlKollektivetLivet,
       crawlSlaktkyrkan,
       crawlHus7,
       crawlFasching,
       crawlNalen,
       crawlFylkingen,
       crawlSlakthuset,
       crawlFallan,
       crawlLandet,
       crawlMosebacke,
       crawlKagelbanan,
       crawlPetSounds,
       crawlDebaser
     ];

     const results = [];
     for (const crawl of crawlers) {
       try {
         const result = await crawl();
         results.push(result);
       } catch (error) {
         console.error(`Crawler failed:`, error);
         results.push({ success: 0, failed: 1 });
       }
     }

     const totals = results.reduce(
       (acc, r) => ({ success: acc.success + r.success, failed: acc.failed + r.failed }),
       { success: 0, failed: 0 }
     );

     console.log(`All venues crawled: ${totals.success} success, ${totals.failed} failed`);
     return totals;
   }
   ```

3. **CRITICAL: During execution, refine each venue's configuration:**
   - Visit each venue's website in browser
   - Inspect HTML structure with DevTools
   - Update selectors to match actual structure
   - Test crawler extracts events correctly
   - If venue uses JavaScript, set usesJavaScript: true
   - Document any special handling needed in comments

4. Handle common venue website patterns:
   - Some venues may not have events page (check homepage)
   - Some may redirect to Facebook events (note in comments, skip crawling)
   - Some may use calendar widgets (may need Playwright)
   - Some may be defunct (Kägelbanan - "if still exists" per PROJECT.md)

Note: Venue websites are HIGHLY VARIABLE. This task requires significant manual refinement during execution. Expect to spend 5-10 minutes per venue inspecting page structure and adjusting selectors. Some venues may be uncrawlable (Facebook only, no events page) - document these cases.
  </action>
  <verify>
Test each venue crawler individually:
  for venue in kollektivet-livet slaktkyrkan hus7 fasching nalen fylkingen slakthuset fallan landet mosebacke kagelbanan pet-sounds debaser; do
    echo "Testing $venue..."
    tsx -e "import { crawl$(echo $venue | sed 's/-//g' | sed 's/^./\U&/g') } from './src/crawlers/venues/$venue.js'; await crawl$(echo $venue | sed 's/-//g' | sed 's/^./\U&/g')();"
  done

Expected: Each runs without crashing (success count may be 0 if no events listed)

Test crawlAllVenues:
  tsx -e "import { crawlAllVenues } from './src/crawlers/venues/index.js'; await crawlAllVenues();"

Expected: Completes all 13 venues, logs total success/failed

Verify database has venue events:
  docker exec -it $(docker-compose ps -q postgres) psql -U dev -d stockholm_events -c "
    SELECT source_platform, COUNT(*)
    FROM events
    WHERE source_platform = 'venue-direct'
    GROUP BY source_platform;
  "
  Expected: venue-direct has some events (exact count depends on current venue schedules)
  </verify>
  <done>
- All 13 venue crawlers implemented with individual files
- Each venue configuration refined based on actual website structure
- venues/index.ts exports all crawlers and convenience function
- crawlAllVenues runs all venue crawlers sequentially
- Uncrawlable venues (Facebook-only, defunct) documented in code comments
- At least 5-8 venues successfully extracting events
- Events stored with source_platform = 'venue-direct'
  </done>
</task>

<task type="auto">
  <name>Task 3: Add health check monitoring for venue crawlers</name>
  <files>
    src/crawlers/venues/health-check.ts
  </files>
  <action>
Implement health monitoring following RESEARCH.md Open Question 2 (venue scraping reliability):

1. Create VenueHealthStatus interface:
   ```typescript
   export interface VenueHealthStatus {
     venue: string;
     status: 'healthy' | 'warning' | 'failing';
     lastSuccessfulCrawl: Date | null;
     recentSuccessRate: number;  // 0-100
     averageEventsPerCrawl: number;
     lastError: string | null;
   }
   ```

2. Implement checkVenueHealth function:
   ```typescript
   import { db } from '../../db/client.js';
   import { events } from '../../db/schema.js';
   import { sql, and, eq, gte } from 'drizzle-orm';

   export async function checkVenueHealth(venueName: string): Promise<VenueHealthStatus> {
     // Check recent events from this venue
     const thirtyDaysAgo = new Date();
     thirtyDaysAgo.setDate(thirtyDaysAgo.getDate() - 30);

     const recentEvents = await db.select()
       .from(events)
       .where(and(
         eq(events.venue, venueName),
         eq(events.sourcePlatform, 'venue-direct'),
         gte(events.createdAt, thirtyDaysAgo)
       ));

     // Calculate health metrics
     const eventCount = recentEvents.length;
     const averageEventsPerCrawl = eventCount / 4;  // Assume weekly crawls = 4 in 30 days

     // Determine status
     let status: 'healthy' | 'warning' | 'failing';
     if (eventCount === 0) {
       status = 'failing';
     } else if (averageEventsPerCrawl < 2) {
       status = 'warning';
     } else {
       status = 'healthy';
     }

     return {
       venue: venueName,
       status,
       lastSuccessfulCrawl: recentEvents[0]?.createdAt || null,
       recentSuccessRate: eventCount > 0 ? 100 : 0,  // Simplified - would need crawl logs for accurate rate
       averageEventsPerCrawl,
       lastError: null  // Would need error logging system
     };
   }

   export async function checkAllVenuesHealth(): Promise<VenueHealthStatus[]> {
     const venues = [
       'Kollektivet Livet', 'Slaktkyrkan', 'Hus 7', 'Fasching', 'Nalen',
       'Fylkingen', 'Slakthuset', 'Fållan', 'Landet', 'Mosebacke',
       'Kägelbanan', 'Pet Sounds', 'Debaser'
     ];

     const results = [];
     for (const venue of venues) {
       const health = await checkVenueHealth(venue);
       results.push(health);
     }

     return results;
   }

   export function formatHealthReport(statuses: VenueHealthStatus[]): string {
     let report = 'Venue Crawler Health Report\n';
     report += '='.repeat(50) + '\n\n';

     for (const status of statuses) {
       const icon = status.status === 'healthy' ? '✓' :
                    status.status === 'warning' ? '⚠' : '✗';

       report += `${icon} ${status.venue}\n`;
       report += `  Status: ${status.status}\n`;
       report += `  Avg events/crawl: ${status.averageEventsPerCrawl.toFixed(1)}\n`;
       report += `  Last crawl: ${status.lastSuccessfulCrawl?.toISOString() || 'never'}\n`;
       report += '\n';
     }

     const failing = statuses.filter(s => s.status === 'failing').length;
     const warning = statuses.filter(s => s.status === 'warning').length;
     const healthy = statuses.filter(s => s.status === 'healthy').length;

     report += `Summary: ${healthy} healthy, ${warning} warning, ${failing} failing\n`;

     return report;
   }
   ```

3. Add CLI command to check health:
   ```typescript
   // Can be run via: tsx -e "import { checkAllVenuesHealth, formatHealthReport } from './src/crawlers/venues/health-check.js'; const statuses = await checkAllVenuesHealth(); console.log(formatHealthReport(statuses));"
   ```

Note: Health checks help detect when venue websites change structure (Pitfall 7). If a venue that was returning 5-10 events per crawl suddenly returns 0, selectors likely need updating.
  </action>
  <verify>
Generate health report:
  cat > test-health.ts << 'EOF'
import { checkAllVenuesHealth, formatHealthReport } from './src/crawlers/venues/health-check.js';

const statuses = await checkAllVenuesHealth();
console.log(formatHealthReport(statuses));

// Check logic
console.log('\nHealth check logic test:');
statuses.forEach(s => {
  const expectation =
    s.averageEventsPerCrawl === 0 ? 'failing' :
    s.averageEventsPerCrawl < 2 ? 'warning' : 'healthy';

  console.log(`${s.venue}: ${s.status === expectation ? 'PASS' : 'FAIL'}`);
});
EOF

  tsx test-health.ts
  Expected: Report shows health status for all 13 venues

After running venue crawlers, verify health improves:
  tsx -e "import { crawlAllVenues } from './src/crawlers/venues/index.js'; await crawlAllVenues();"
  tsx test-health.ts
  Expected: More venues show 'healthy' status after crawl
  </verify>
  <done>
- checkVenueHealth analyzes recent crawl results per venue
- Health status determined by average events per crawl
- checkAllVenuesHealth provides full report across all venues
- formatHealthReport creates human-readable output
- Health monitoring helps detect broken venue crawlers early
- CLI command available for manual health checks
  </done>
</task>

</tasks>

<verification>
End-to-end venue crawler verification:

1. Full venue crawl:
   ```bash
   tsx -e "import { crawlAllVenues } from './src/crawlers/venues/index.js'; await crawlAllVenues();"
   ```
   Expected: Completes all 13 venues, some with events

2. Database verification:
   ```sql
   -- Check venue events
   SELECT COUNT(*) FROM events WHERE source_platform = 'venue-direct';
   -- Expected: > 10 events (depends on current schedules)

   -- Breakdown by venue
   SELECT venue, COUNT(*) as event_count
   FROM events
   WHERE source_platform = 'venue-direct'
   GROUP BY venue
   ORDER BY event_count DESC;
   -- Expected: Several venues with 1-10 events each

   -- Verify normalized venue names
   SELECT DISTINCT venue FROM events WHERE source_platform = 'venue-direct';
   -- Expected: All match priority venue names exactly
   ```

3. Health check:
   ```bash
   tsx -e "import { checkAllVenuesHealth, formatHealthReport } from './src/crawlers/venues/health-check.js'; console.log(formatHealthReport(await checkAllVenuesHealth()));"
   ```
   Expected: At least 5-8 venues showing 'healthy' or 'warning', documented reasons for 'failing' venues
</verification>

<success_criteria>
- BaseVenueCrawler template works for both static and dynamic venue sites
- All 13 priority venues have dedicated crawler configurations
- At least 5-8 venues successfully extract events (others documented as uncrawlable)
- Events stored with source_platform = 'venue-direct'
- Venue names are normalized (match priority venue list exactly)
- Health check system monitors venue crawler reliability
- Health report identifies failing venues for investigation
- crawlAllVenues convenience function runs all venue crawlers
- Documentation explains which venues are uncrawlable and why
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-foundation-multi-platform-aggregation/01-06-SUMMARY.md`
</output>
