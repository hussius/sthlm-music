---
phase: 01-data-foundation-multi-platform-aggregation
plan: 08
type: execute
wave: 4
depends_on:
  - 01-03
  - 01-04
  - 01-05
  - 01-06
  - 01-07
files_modified:
  - src/scheduling/jobs.ts
  - src/scheduling/processors.ts
  - src/scheduling/monitoring.ts
  - src/scheduling/cleanup.ts
autonomous: false
requirements:
  - DATA-06

must_haves:
  truths:
    - System crawls all sources daily at 3 AM Stockholm time automatically
    - Failed crawls trigger alerts within 5 minutes
    - Old events (>12 months) are removed weekly automatically
    - Job status visible via monitoring dashboard
  artifacts:
    - path: "src/scheduling/jobs.ts"
      provides: "BullMQ job definitions for all crawlers"
      exports: ["setupCrawlJobs", "setupCleanupJob"]
      min_lines: 80
    - path: "src/scheduling/processors.ts"
      provides: "Job execution handlers"
      exports: ["processCrawlJob", "processCleanupJob"]
      min_lines: 60
    - path: "src/scheduling/monitoring.ts"
      provides: "Job monitoring and alerting"
      exports: ["monitorJobs", "checkJobHealth"]
      min_lines: 80
    - path: "src/scheduling/cleanup.ts"
      provides: "12-month rolling window maintenance"
      exports: ["cleanupOldEvents"]
      min_lines: 40
  key_links:
    - from: "src/scheduling/jobs.ts"
      to: "bullmq"
      via: "creates job queue with Redis"
      pattern: "new Queue"
    - from: "src/scheduling/processors.ts"
      to: "src/crawlers/*"
      via: "executes crawler functions"
      pattern: "await crawl"
    - from: "src/scheduling/monitoring.ts"
      to: "src/scheduling/jobs.ts"
      via: "monitors job queue status"
      pattern: "getJobs"
---

<objective>
Implement automated job scheduling with BullMQ to run all crawlers daily at 3 AM Stockholm time, maintain the 12-month rolling event window, monitor job health, and alert on failures. This ensures the system operates autonomously without manual intervention.

Purpose: Requirements DATA-01 through DATA-04 specify "daily" crawling. This plan automates that daily execution with Redis-backed job queues, retry logic, failure alerting, and cleanup of old events to maintain the 12-month window.

Output: BullMQ job scheduler with daily crawl jobs, weekly cleanup job, monitoring dashboard, failure alerting system, and integration with all crawlers.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-data-foundation-multi-platform-aggregation/01-RESEARCH.md
@.planning/phases/01-data-foundation-multi-platform-aggregation/01-03-SUMMARY.md
@.planning/phases/01-data-foundation-multi-platform-aggregation/01-04-SUMMARY.md
@.planning/phases/01-data-foundation-multi-platform-aggregation/01-05-SUMMARY.md
@.planning/phases/01-data-foundation-multi-platform-aggregation/01-06-SUMMARY.md
@.planning/phases/01-data-foundation-multi-platform-aggregation/01-07-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Set up BullMQ job queue with daily crawl jobs</name>
  <files>
    src/scheduling/jobs.ts
    src/scheduling/processors.ts
  </files>
  <action>
Implement job scheduling following RESEARCH.md Pattern 4 (BullMQ Scheduled Crawls):

**Part A: Job definitions (jobs.ts)**

1. Create job queue and define all crawl jobs:
   ```typescript
   import { Queue, QueueEvents } from 'bullmq';
   import { config } from '../config/env.js';

   // Create queue with Redis connection
   export const crawlQueue = new Queue('event-crawls', {
     connection: {
       host: new URL(config.REDIS_URL).hostname,
       port: parseInt(new URL(config.REDIS_URL).port) || 6379
     },
     defaultJobOptions: {
       attempts: 3,
       backoff: {
         type: 'exponential',
         delay: 60000  // Start with 1 minute, exponential backoff
       },
       removeOnComplete: {
         age: 86400  // Keep completed jobs for 24 hours
       },
       removeOnFail: {
         age: 604800  // Keep failed jobs for 7 days
       }
     }
   });

   export async function setupCrawlJobs(): Promise<void> {
     // Schedule Ticketmaster crawl at 3:00 AM Stockholm time
     await crawlQueue.add(
       'ticketmaster-crawl',
       { source: 'ticketmaster' },
       {
         repeat: {
           pattern: '0 3 * * *',
           tz: 'Europe/Stockholm'
         },
         jobId: 'ticketmaster-daily'
       }
     );

     // Schedule AXS crawl at 3:15 AM (stagger to avoid resource contention)
     await crawlQueue.add(
       'axs-crawl',
       { source: 'axs' },
       {
         repeat: {
           pattern: '15 3 * * *',
           tz: 'Europe/Stockholm'
         },
         jobId: 'axs-daily'
       }
     );

     // Schedule DICE crawl at 3:30 AM
     await crawlQueue.add(
       'dice-crawl',
       { source: 'dice' },
       {
         repeat: {
           pattern: '30 3 * * *',
           tz: 'Europe/Stockholm'
         },
         jobId: 'dice-daily'
       }
     );

     // Schedule venue crawls at 4:00 AM (after major platforms)
     await crawlQueue.add(
       'venues-crawl',
       { source: 'venues' },
       {
         repeat: {
           pattern: '0 4 * * *',
           tz: 'Europe/Stockholm'
         },
         jobId: 'venues-daily'
       }
     );

     console.log('Scheduled daily crawl jobs');
   }

   export async function setupCleanupJob(): Promise<void> {
     // Schedule cleanup every Sunday at 4:00 AM
     await crawlQueue.add(
       'cleanup',
       { task: 'remove-old-events' },
       {
         repeat: {
           pattern: '0 4 * * 0',
           tz: 'Europe/Stockholm'
         },
         jobId: 'cleanup-weekly'
       }
     );

     console.log('Scheduled weekly cleanup job');
   }

   // Initialize queue events for monitoring
   export const queueEvents = new QueueEvents('event-crawls', {
     connection: {
       host: new URL(config.REDIS_URL).hostname,
       port: parseInt(new URL(config.REDIS_URL).port) || 6379
     }
   });
   ```

**Part B: Job processors (processors.ts)**

2. Implement job execution handlers:
   ```typescript
   import { Worker, Job } from 'bullmq';
   import { config } from '../config/env.js';
   import { crawlTicketmaster } from '../crawlers/ticketmaster.js';
   import { crawlAXS } from '../crawlers/axs.js';
   import { crawlDICE } from '../crawlers/dice.js';
   import { crawlAllVenues } from '../crawlers/venues/index.js';
   import { cleanupOldEvents } from './cleanup.js';

   export async function processCrawlJob(job: Job): Promise<any> {
     const { source } = job.data;

     console.log(`Starting ${source} crawl job...`);

     try {
       let result;

       switch (source) {
         case 'ticketmaster':
           result = await crawlTicketmaster();
           break;

         case 'axs':
           result = await crawlAXS();
           break;

         case 'dice':
           result = await crawlDICE();
           break;

         case 'venues':
           result = await crawlAllVenues();
           break;

         case 'cleanup':
           result = await cleanupOldEvents();
           break;

         default:
           throw new Error(`Unknown crawler source: ${source}`);
       }

       console.log(`${source} crawl completed:`, result);
       return {
         success: true,
         source,
         eventsCollected: result.success,
         failed: result.failed,
         completedAt: new Date().toISOString()
       };

     } catch (error: any) {
       console.error(`${source} crawl failed:`, error);
       throw error;  // Re-throw to trigger retry
     }
   }

   export async function processCleanupJob(job: Job): Promise<any> {
     console.log('Starting cleanup job...');

     try {
       const result = await cleanupOldEvents();

       console.log('Cleanup completed:', result);
       return {
         success: true,
         eventsRemoved: result.deleted,
         completedAt: new Date().toISOString()
       };

     } catch (error: any) {
       console.error('Cleanup failed:', error);
       throw error;
     }
   }

   // Create worker to process jobs
   export function createWorker() {
     const worker = new Worker(
       'event-crawls',
       async (job) => {
         if (job.name === 'cleanup') {
           return processCleanupJob(job);
         } else {
           return processCrawlJob(job);
         }
       },
       {
         connection: {
           host: new URL(config.REDIS_URL).hostname,
           port: parseInt(new URL(config.REDIS_URL).port) || 6379
         },
         concurrency: 1  // Process one job at a time
       }
     );

     // Event handlers
     worker.on('completed', (job) => {
       console.log(`Job ${job.id} (${job.name}) completed successfully`);
     });

     worker.on('failed', (job, err) => {
       console.error(`Job ${job?.id} (${job?.name}) failed after ${job?.attemptsMade} attempts:`, err);
     });

     return worker;
   }
   ```

Note: Jobs are staggered (3:00, 3:15, 3:30, 4:00) to avoid overwhelming the system with parallel crawls. Venues run last to benefit from data already collected from major platforms (helps deduplication). Cleanup runs on Sundays when traffic is typically lower.
  </action>
  <verify>
Test job scheduling:
  cat > test-jobs.ts << 'EOF'
import { setupCrawlJobs, setupCleanupJob, crawlQueue } from './src/scheduling/jobs.js';
import { createWorker } from './src/scheduling/processors.js';

// Set up jobs
await setupCrawlJobs();
await setupCleanupJob();

// Check scheduled jobs
const repeatableJobs = await crawlQueue.getRepeatableJobs();
console.log(`Scheduled ${repeatableJobs.length} repeatable jobs:`);
repeatableJobs.forEach(job => {
  console.log(`  - ${job.id}: ${job.pattern} (${job.tz})`);
});

// Verify all sources scheduled
const expected = ['ticketmaster-daily', 'axs-daily', 'dice-daily', 'venues-daily', 'cleanup-weekly'];
const scheduled = repeatableJobs.map(j => j.id);
const allScheduled = expected.every(id => scheduled.includes(id));

console.log('\nAll jobs scheduled:', allScheduled ? 'PASS' : 'FAIL');

// Clean up (remove test jobs)
for (const job of repeatableJobs) {
  await crawlQueue.removeRepeatableByKey(job.key);
}
EOF

  tsx test-jobs.ts
  Expected: 5 repeatable jobs scheduled (4 crawls + 1 cleanup), all at correct times

Test worker processing:
  cat > test-worker.ts << 'EOF'
import { crawlQueue } from './src/scheduling/jobs.js';
import { createWorker } from './src/scheduling/processors.js';

// Start worker
const worker = createWorker();

// Add test job (immediate execution)
const job = await crawlQueue.add('test-crawl', { source: 'ticketmaster' });
console.log('Added test job:', job.id);

// Wait for completion (with timeout)
await job.waitUntilFinished(queueEvents, 120000);  // 2 minute timeout

console.log('Test job completed successfully');

await worker.close();
EOF

  tsx test-worker.ts
  Expected: Worker processes job, Ticketmaster crawl runs, job completes
  </verify>
  <done>
- crawlQueue created with Redis connection
- Four daily crawl jobs scheduled (Ticketmaster, AXS, DICE, venues)
- Jobs staggered to avoid resource contention
- Weekly cleanup job scheduled for Sundays
- Job processor handles all crawler execution
- Worker created with proper event handlers
- Retry logic configured (3 attempts, exponential backoff)
- Completed/failed jobs retained for debugging
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement 12-month rolling window cleanup</name>
  <files>
    src/scheduling/cleanup.ts
  </files>
  <action>
Implement cleanup job following RESEARCH.md code example (Rolling Window Cleanup):

1. Create cleanupOldEvents function:
   ```typescript
   import { db } from '../db/client.js';
   import { events, reviewQueue } from '../db/schema.js';
   import { lt } from 'drizzle-orm';

   export async function cleanupOldEvents(): Promise<{ deleted: number; reviewQueueCleaned: number }> {
     // Calculate cutoff date (12 months ago)
     const twelveMonthsAgo = new Date();
     twelveMonthsAgo.setMonth(twelveMonthsAgo.getMonth() - 12);

     console.log(`Cleaning up events older than ${twelveMonthsAgo.toISOString()}`);

     // Delete old events
     const eventResult = await db.delete(events)
       .where(lt(events.date, twelveMonthsAgo))
       .returning({ id: events.id });

     const deletedCount = eventResult.length;

     console.log(`Deleted ${deletedCount} events older than 12 months`);

     // Clean up orphaned review queue entries
     // (events that were deleted but review queue entries remain)
     const reviewResult = await db.delete(reviewQueue)
       .where(lt(reviewQueue.createdAt, twelveMonthsAgo))
       .returning({ id: reviewQueue.id });

     const reviewCleaned = reviewResult.length;

     console.log(`Cleaned ${reviewCleaned} old review queue entries`);

     return { deleted: deletedCount, reviewQueueCleaned: reviewCleaned };
   }

   export async function getEventCountsByAge(): Promise<{
     total: number;
     withinWindow: number;
     pastWindow: number;
   }> {
     const twelveMonthsAgo = new Date();
     twelveMonthsAgo.setMonth(twelveMonthsAgo.getMonth() - 12);

     const allEvents = await db.select().from(events);
     const total = allEvents.length;

     const withinWindow = allEvents.filter(e => e.date >= twelveMonthsAgo).length;
     const pastWindow = total - withinWindow;

     return { total, withinWindow, pastWindow };
   }
   ```

2. Add manual cleanup trigger for testing:
   ```typescript
   export async function forceCleanup(): Promise<void> {
     console.log('Force cleanup triggered');

     const counts = await getEventCountsByAge();
     console.log('Before cleanup:', counts);

     const result = await cleanupOldEvents();
     console.log('Cleanup result:', result);

     const countsAfter = await getEventCountsByAge();
     console.log('After cleanup:', countsAfter);
   }
   ```

Note: Cleanup runs weekly (Sundays at 4 AM) to maintain the 12-month rolling window per requirement DATA-06. This prevents the database from growing indefinitely and ensures users only see upcoming events.
  </action>
  <verify>
Test cleanup logic:
  cat > test-cleanup.ts << 'EOF'
import { cleanupOldEvents, getEventCountsByAge } from './src/scheduling/cleanup.js';
import { db } from './src/db/client.js';
import { events } from './src/db/schema.js';

// Insert test event in the past (13 months ago)
const pastDate = new Date();
pastDate.setMonth(pastDate.getMonth() - 13);

await db.insert(events).values({
  name: 'Past Event',
  artist: 'Past Artist',
  venue: 'Test Venue',
  date: pastDate,
  genre: 'rock',
  ticketUrl: 'https://example.com',
  sourceId: 'test-past',
  sourcePlatform: 'ticketmaster'
});

console.log('Inserted past event');

// Check counts before cleanup
const before = await getEventCountsByAge();
console.log('Before cleanup:', before);
console.log('Has past events:', before.pastWindow > 0 ? 'YES' : 'NO');

// Run cleanup
const result = await cleanupOldEvents();
console.log('Cleanup result:', result);

// Check counts after cleanup
const after = await getEventCountsByAge();
console.log('After cleanup:', after);

console.log('Cleanup removed past events:', after.pastWindow === 0 ? 'PASS' : 'FAIL');
EOF

  tsx test-cleanup.ts
  Expected: Past event inserted, cleanup removes it, pastWindow = 0 after cleanup

Verify 12-month window maintained:
  docker exec -it $(docker-compose ps -q postgres) psql -U dev -d stockholm_events -c "
    SELECT
      MIN(date) as earliest,
      MAX(date) as latest,
      AGE(MAX(date), MIN(date)) as range
    FROM events;
  "
  Expected: Range <= 12 months
  </verify>
  <done>
- cleanupOldEvents deletes events older than 12 months
- Review queue entries for deleted events also cleaned
- getEventCountsByAge provides visibility into event age distribution
- forceCleanup available for manual testing/emergency cleanup
- Cleanup job scheduled to run weekly on Sundays
- 12-month rolling window maintained automatically
  </done>
</task>

<task type="auto">
  <name>Task 3: Add job monitoring and failure alerting</name>
  <files>
    src/scheduling/monitoring.ts
  </files>
  <action>
Implement monitoring and alerting:

1. Create job health monitoring:
   ```typescript
   import { Queue, QueueEvents } from 'bullmq';
   import { crawlQueue, queueEvents } from './jobs.js';

   export interface JobHealthStatus {
     jobName: string;
     lastRun: Date | null;
     lastStatus: 'completed' | 'failed' | 'never_run';
     failureCount: number;
     successCount: number;
     avgDuration: number;  // milliseconds
   }

   export async function checkJobHealth(): Promise<JobHealthStatus[]> {
     const repeatableJobs = await crawlQueue.getRepeatableJobs();

     const statuses: JobHealthStatus[] = [];

     for (const job of repeatableJobs) {
       // Get completed jobs for this repeatable job
       const completed = await crawlQueue.getJobs(['completed'], 0, 10, false);
       const failed = await crawlQueue.getJobs(['failed'], 0, 10, false);

       const jobCompleted = completed.filter(j => j.name === job.id);
       const jobFailed = failed.filter(j => j.name === job.id);

       // Calculate stats
       const lastRun = jobCompleted[0]?.finishedOn ?
         new Date(jobCompleted[0].finishedOn) : null;

       const lastStatus =
         jobCompleted.length === 0 && jobFailed.length === 0 ? 'never_run' :
         jobCompleted.length > 0 ? 'completed' : 'failed';

       const avgDuration = jobCompleted.length > 0 ?
         jobCompleted.reduce((sum, j) =>
           sum + (j.finishedOn! - j.processedOn!), 0) / jobCompleted.length : 0;

       statuses.push({
         jobName: job.id,
         lastRun,
         lastStatus,
         failureCount: jobFailed.length,
         successCount: jobCompleted.length,
         avgDuration
       });
     }

     return statuses;
   }

   export async function monitorJobs(): Promise<void> {
     // Listen for job failures and alert
     queueEvents.on('failed', async ({ jobId, failedReason }) => {
       const job = await crawlQueue.getJob(jobId);
       const attempts = job?.attemptsMade || 0;

       console.error(`JOB FAILED: ${job?.name} (attempt ${attempts}/3)`);
       console.error(`Reason: ${failedReason}`);

       // If all retries exhausted, send alert
       if (attempts >= 3) {
         await sendAlert({
           type: 'job_failure',
           jobName: job?.name || 'unknown',
           jobId,
           reason: failedReason,
           timestamp: new Date()
         });
       }
     });

     // Listen for job completions
     queueEvents.on('completed', async ({ jobId, returnvalue }) => {
       const job = await crawlQueue.getJob(jobId);
       console.log(`Job completed: ${job?.name}`, returnvalue);
     });

     console.log('Job monitoring started');
   }

   interface Alert {
     type: 'job_failure' | 'health_check_failure';
     jobName: string;
     jobId?: string;
     reason?: string;
     timestamp: Date;
   }

   async function sendAlert(alert: Alert): Promise<void> {
     // Log to console (in production, would send to Slack/email/PagerDuty)
     console.error('========================================');
     console.error('ALERT: Job Failure');
     console.error(`Job: ${alert.jobName}`);
     console.error(`Reason: ${alert.reason}`);
     console.error(`Time: ${alert.timestamp.toISOString()}`);
     console.error('Action required: Investigate job failure');
     console.error('========================================');

     // TODO: Integrate with actual alerting system (Slack, email, etc.)
     // Example: await sendSlackMessage(webhookUrl, alertMessage);
   }

   export function formatHealthReport(statuses: JobHealthStatus[]): string {
     let report = 'Job Health Report\n';
     report += '='.repeat(60) + '\n\n';

     for (const status of statuses) {
       const icon = status.lastStatus === 'completed' ? '✓' :
                    status.lastStatus === 'failed' ? '✗' : '○';

       report += `${icon} ${status.jobName}\n`;
       report += `  Last run: ${status.lastRun?.toISOString() || 'never'}\n`;
       report += `  Status: ${status.lastStatus}\n`;
       report += `  Success/Fail: ${status.successCount}/${status.failureCount}\n`;
       report += `  Avg duration: ${(status.avgDuration / 1000).toFixed(1)}s\n`;
       report += '\n';
     }

     return report;
   }

   export async function getQueueMetrics() {
     const waiting = await crawlQueue.getWaitingCount();
     const active = await crawlQueue.getActiveCount();
     const completed = await crawlQueue.getCompletedCount();
     const failed = await crawlQueue.getFailedCount();

     return { waiting, active, completed, failed };
   }
   ```

2. Add startup script that initializes monitoring:
   ```typescript
   // src/index.ts - update to start monitoring
   import { monitorJobs } from './scheduling/monitoring.js';
   import { createWorker } from './scheduling/processors.js';

   export async function startSystem() {
     // Start worker
     const worker = createWorker();

     // Start monitoring
     await monitorJobs();

     console.log('Stockholm Events Crawler system started');
     console.log('Worker and monitoring active');
   }
   ```

Note: Real production alerting would integrate with Slack webhook, SendGrid email, or PagerDuty. This implementation logs alerts to console with clear ERROR markers that monitoring systems can detect.
  </action>
  <verify>
Test monitoring:
  cat > test-monitoring.ts << 'EOF'
import { checkJobHealth, formatHealthReport, getQueueMetrics, monitorJobs } from './src/scheduling/monitoring.js';
import { setupCrawlJobs } from './src/scheduling/jobs.js';

// Set up jobs
await setupCrawlJobs();

// Start monitoring (in background)
await monitorJobs();

// Check health
const health = await checkJobHealth();
console.log(formatHealthReport(health));

// Check queue metrics
const metrics = await getQueueMetrics();
console.log('Queue metrics:', metrics);

console.log('\nMonitoring test:', health.length > 0 ? 'PASS' : 'FAIL');
EOF

  tsx test-monitoring.ts
  Expected: Health report shows all scheduled jobs, monitoring starts

Test failure alerting:
  # Modify a crawler to fail intentionally, run job, verify alert logged
  # Should see "ALERT: Job Failure" in console with details
  </verify>
  <done>
- checkJobHealth analyzes job execution history
- Job failures logged with clear alert markers
- Alerts triggered after all retries exhausted
- monitorJobs listens for job events continuously
- formatHealthReport provides human-readable status
- getQueueMetrics shows queue activity
- System ready for integration with real alerting (Slack/email)
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Automated job scheduling system with BullMQ that:
- Runs Ticketmaster, AXS, DICE, and venue crawlers daily at 3-4 AM Stockholm time
- Cleans up events older than 12 months every Sunday
- Monitors job execution and alerts on failures
- Provides health check reporting
  </what-built>
  <how-to-verify>
1. Verify job system is running:
   ```bash
   cd /Users/hussmikael/agents-hackathon
   npm run dev  # Start system with monitoring
   ```
   Expected: Logs show "Job monitoring started", "Worker active"

2. Check scheduled jobs:
   ```bash
   tsx -e "import { setupCrawlJobs, setupCleanupJob, crawlQueue } from './src/scheduling/jobs.js'; await setupCrawlJobs(); await setupCleanupJob(); const jobs = await crawlQueue.getRepeatableJobs(); console.log('Scheduled jobs:', jobs.map(j => ({ id: j.id, pattern: j.pattern, tz: j.tz })));"
   ```
   Expected: 5 jobs listed (ticketmaster, axs, dice, venues, cleanup) with correct cron patterns

3. Trigger test crawl immediately:
   ```bash
   tsx -e "import { crawlQueue } from './src/scheduling/jobs.js'; const job = await crawlQueue.add('test', { source: 'ticketmaster' }); console.log('Test job added:', job.id);"
   ```
   Expected: Job executes within 1-2 minutes, completion logged

4. View health report:
   ```bash
   tsx -e "import { checkJobHealth, formatHealthReport } from './src/scheduling/monitoring.js'; const health = await checkJobHealth(); console.log(formatHealthReport(health));"
   ```
   Expected: Shows health status for all jobs

5. Verify cleanup job works:
   ```bash
   tsx -e "import { forceCleanup } from './src/scheduling/cleanup.js'; await forceCleanup();"
   ```
   Expected: Shows events deleted (if any >12 months old exist)

6. Check Redis connection:
   ```bash
   docker-compose ps | grep redis
   ```
   Expected: Redis container running

7. Verify daily crawls will run automatically:
   - Leave system running overnight OR
   - Manually check next scheduled run time in job queue

**Accept if:**
- All 5 jobs are scheduled with correct times
- Test job executes successfully
- Health report shows job statuses
- Cleanup removes old events
- No errors in logs
  </how-to-verify>
  <resume-signal>
Type "approved" when job scheduling system is verified working, or describe any issues found.
  </resume-signal>
</task>

</tasks>

<verification>
End-to-end scheduling system verification:

1. Full system startup:
   ```bash
   docker-compose up -d  # Start PostgreSQL and Redis
   npm run dev  # Start worker and monitoring
   ```
   Expected: System starts, jobs scheduled, monitoring active

2. Verify scheduled execution times:
   ```sql
   -- After first automated run (wait until 3-4 AM or trigger manually)
   SELECT source_platform, MAX(created_at) as last_crawl
   FROM events
   GROUP BY source_platform;
   -- Expected: All platforms updated recently
   ```

3. Verify 12-month window maintained:
   ```sql
   SELECT
     COUNT(*) as total,
     MIN(date) as earliest,
     MAX(date) as latest,
     EXTRACT(MONTH FROM AGE(MAX(date), MIN(date))) as months
   FROM events;
   -- Expected: months <= 12
   ```

4. Test failure recovery:
   - Temporarily break a crawler (invalid URL)
   - Wait for scheduled run or trigger manually
   - Verify: Job fails, retries 3 times, alert logged
   - Fix crawler, verify next run succeeds
</verification>

<success_criteria>
- BullMQ queue configured with Redis connection
- Four daily crawl jobs scheduled at staggered times (3:00-4:00 AM Stockholm)
- Weekly cleanup job scheduled (Sundays at 4 AM)
- Jobs retry 3 times with exponential backoff on failure
- Job failures trigger alerts after all retries exhausted
- Worker processes jobs successfully
- Cleanup maintains 12-month rolling window
- Health monitoring provides job status visibility
- System operates autonomously without manual intervention
- All crawlers run daily per requirements DATA-01 through DATA-04
- Old events removed automatically per requirement DATA-06
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-foundation-multi-platform-aggregation/01-08-SUMMARY.md`
</output>
